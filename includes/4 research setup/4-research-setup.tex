\chapter{Forschungsaufbau der Fallstudie}\label{forschungsaufbau-der-fallstudie}

In dieser Arbeit soll eine Fallstudie durchgeführt werden. Entsprechend
der Fallstudienmethodik wird in diesem Kapitel der Forschungsaufbau
zunächst in einem Forschungsprotokoll festgehalten
(gothlichFallstudienAlsForschungsmethode2003 S. 8,
millsEncyclopediaCaseStudy2010 S. 69f). Wichtige Komponenten sind dabei
zunächst eine Forschungsfrage, welche die Problemstellung und die Ziele
des Handelns aufzeigt. Dann wird eine Hypothese über die Antwort
aufgestellt. Es werden die einzelnen Fälle bzw. die Analyseeinheiten
differenziert, ausgewählt und vorgestellt. Die Daten aus diesen
Analyseeinheiten werden in Verbindung zu den Hypothesen gesetzt und es
werden Kriterien zur Interpretation der Daten aufgestellt
(millsEncyclopediaCaseStudy2010 S. 69f).

\section{Forschungsfrage}\label{forschungsfrage}

Eine klare Forschungsfrage ist essenziell für die Fallstudienforschung,
um die Problemstellung und Ziele des Handels aufzuzeigen
(dubeRigorInformationSystems2003 S. 605, millsEncyclopediaCaseStudy2010
S. 70, dubeRigorInformationSystems2003 S. 607). In der
Fallstudienforschung können Forschungsfragen explorativ, deskriptiv und
explanativ sowie eine Kombination aller drei Arten sein
(dubeRigorInformationSystems2003 S. 605).

In dieser Arbeit soll das Thema der Softwarekomplexitätsmetriken
deskriptiv (Frage nach dem Wie) behandelt werden. Es soll beschrieben
werden, wie sich Komplexitätsmetriken im Vergleich zu
Aufwandsabschätzungen verhalten.

\section{Hypothesen}\label{hypothesen}

Vor der Beantwortung der Forschungsfrage werden zunächst Hypothesen über
mögliche Antworten aufgestellt
(gothlichFallstudienAlsForschungsmethode2003 S. 8). Das Aufstellen von
Hypothesen dient der Identifikation der weiteren Forschungsrichtung
(dubeRigorInformationSystems2003 S. 607, millsEncyclopediaCaseStudy2010
S. 70).

In dieser Arbeit wird die Hypothese verfolgt, dass eine eingeschränkte
Korrelation zwischen Softwarekomplexitätsmetriken und
Aufwandsabschätzungen nachgewiesen werden kann. Es ist zu erwarten, dass
Störfaktoren diese Korrelation beeinflussen.

\section{Analyseeinheit}\label{analyseeinheit}

Die Hypothese soll durch die Betrachtung einer Reihe von Fällen (Cases)
als Untersuchungseinheiten validiert oder widerlegt werden.

Im allgemein anerkannten Vorgehen zur Fallstudienforschung sei an dieser
Stelle eine klare Beschreibung der Fälle von Bedeutung
(dubeRigorInformationSystems2003 S. 610).

Auch an die Auswahl der Fälle bestünden Anforderungen. Die Fälle müssen
im Zusammenhang zu der Forschungsfrage gewählt werden, können in diesem
Rahmen aber durchaus willkürlich ausgewählt werden
(millsEncyclopediaCaseStudy2010 S. 72f, yinCaseStudyResearch2014 S. 72).
Eine willkürliche Auswahl der Fälle ist gerade deswegen wichtig, da
insbesondere Extremfälle die gesamte Bandbreite der möglichen
Analyseergebnisse abdecken können. Die willkürliche Auswahl muss jedoch
keinem Zufallsprinzip folgen\footcite[Vgl. ][]{Quelle fehlt (Ausreißer und
  Extremfälle gerade an diesen Extremfällen gelegen, da sie die
  Bandbreite abstecken, innerhalb derer sich die Realität bewegt und
  relevante Phänomene in diesen Fällen am deutlichsten zu Tage treten.)}.
Als Grundvoraussetzung zur Auswahl der Fälle lässt sich sagen, dass ein
möglichst umfassender Zugang zu Dokumenten und Artefakten des Falles
gegeben sein sollte (millsEncyclopediaCaseStudy2010 S. 68). Für eine
möglichst valide Endaussage sei eine Anzahl von vier bis zehn Fällen
anzustreben (gothlichFallstudienAlsForschungsmethode2003 S.
9\footcite[Vgl. ][]{Eisenhardt, Kathleen M. (1989): 'Building Theories from Case
  Study Research', in: Academy of Management Review, Vol. 14, No. 4, S.
  532-550.}, dubeRigorInformationSystems2003 S. 609).

In dieser Arbeit soll die Entwicklung von Softwarekomplexitätsmetriken
betrachtet werden. Also liegt es auf der Hand, die Quelltexte von
Softwareprojekten als Analyseeinheiten zu betrachten. Die
Komplexitätsmetriken sollen mit insgesamt sechs Softwareprojekten
validiert werden. Bei fünf Projekten handelt es sich um Projekte, die
unternehmensintern im Kundenauftrag entwickelt werden bzw. wurden. Bei
einem weiteren Projekt handelt es sich um eine Softwarelösung, die
bereits von über 100 Tausend Organisationen verwendet wird\footcite[Vgl. ][]{GitLabGitLab}.
Es lässt sich also sagen, dass der wirtschaftliche bzw. praktische Bezug
aller Projekte sichergestellt ist. Aufgrund der Positionierung der
ersten fünf Projekte als unternehmensinterne Entwicklungen ist hier des
weiteren der weitgehend unlimitierte und vor allem unverfälschte Zugang
zu Daten der Projekte sichergestellt. Auch bei dem externen Projekt
besteht ein hinreichender Datenzugriff. Somit sind alle zuvor
aufgezeigten Voraussetzungen zur Auswahl der Fälle einer Fallstudie bei
den hier behandelten sechs Fällen bedient.

Im Sinne einer klaren Beschreibung der Fälle sollen einige Metadaten zu
den Fällen gesammelt werden. Die Art der abgefragten Informationen
richtet sich nach einer Arbeit von Sato et al (Sato et al 2006:49f) und
mehreren Befragungen von Experten in dem Unternehmen. Insgesamt konnten
so neun Datenpunkte identifiziert werden.

\begin{itemize}
    \item
      Der \emph{Name} des Projektes: Hier musste teilweise aus Gründen der
      Datensicherheit ein Pseudonym verwendet werden.
    \item
      Eine \emph{Beschreibung}: Diese Beschreibung sollte den
      Verwendungszweck und eine grobe betriebliche Motivation des Projektes
      beinhalten.
    \item
      \emph{Entwicklungsmethode}: Ob das Projekt agil nach Scrum oder einer
      anderen Entwicklungsmethode entwickelt wird, ist ebenfalls relevant
      für die Arbeit.
    \item
      \emph{Programmiersprache}: Welche Programmiersprachen in dem Projekt
      hauptsächlich verwendet werden.
    \item
      \emph{Offshore oder Onshore}: Ob die Entwicklung des Projektes in ein
      anderes Land (offshore) verlegt wurde.
    \item
      \emph{Team Lokation}: Der Hauptsitz des Entwicklungsteams.
    \item
      \emph{Kunden Lokation}: Der Hauptsitz des Kunden.
    \item
      \emph{Erfahrung der Entwickler}: Wie viel Berufserfahrung die
      Entwickler aufweisen.
    \item
      \emph{Erfahrung der Entwickler mit dem Projekt}: Wie viel Erfahrung
      die Entwickler bereits in dem speziellen Projekt haben.
    \end{itemize}

Die Datenpunkte Name, Beschreibung, Entwicklungsmethode, Sprache, Team
Lokation und Kunden Lokation wurden sowohl von Sato et al 2006 und von
Experten innerhalb des Unternehmens als wichtig erachtet. Die Relevanz
der weiteren Punkte begründet sich allein aus unternehmensinternen
Expertenmeinungen.

\section{Datensammlung}\label{datensammlung}

Zu den ausgewählten Quellen werden nun Daten erfasst. Das Ziel der
Datenerfassung ist das Schaffen einer belastbaren Basis, auf der eine
spätere Belegung oder Widerlegung der Hypothese geschehen kann. Für die
Zuverlässigkeit und Validität der getätigten Aussagen sei eine genaue
Beschreibung der Datenquellen unerlässlich\footcite[Vgl. ][]{(dubeRigorInformationSystems2003
  S. 612)} \footcite[Vgl. ][]{1. (p. 381) (dubeRigorInformationSystems2003 S.
  614)} \footcite[Vgl. ][]{Benbasat et al.'s (1987)}. Die Auswahl und
Beschreibung der Datenquellen wird in verschiedenster Literatur
umfassend erläutert\footcite[Vgl. ][]{(gothlichFallstudienAlsForschungsmethode2003
  S. 10); Girtler, Roland (2001): Methoden der Feldforschung, 4., völlig
  neu bearbeitete Auflage, Wien, Köln, Weimar; Lueger, Manfred (2000):
  Grundlagen qualitativer Feldforschung, Me-thodologie, Organisierung,
  Materialanalyse, Wien; Piore, Michael J. (1979): 'Qualitative Research
  Techniques in Econom-ics', in: Administrative Science Quarterly, Vol.
  24 (1979), No. 4, S. 560-569.}.

  Für die Auswahl der Datenquellen kommen verschiedene Quellenformate in
Frage: Im Generellen seien Dokumente jeglicher Art als Quelle
zulässig\footcite[Vgl. ][]{(gothlichFallstudienAlsForschungsmethode2003 S.10)}.
Auch Archivdaten spielen eine wichtige Rolle. So wurde in einer
Metastudie ermittelt, dass in ca. 64\% aller beobachteten Fallstudien
Archivdaten als Quelle eingesetzt wurden\footcite[Vgl. ][]{(dubeRigorInformationSystems2003
  S. 614)}. Hier seien insbesondere Zeitreihendaten von Interesse
\footcite[Vgl. ][]{i. (Benbasatet al. 1987; Eisenhardt1989).
  (dubeRigorInformationSystems2003 S. 612)}. Weiter seien Interviews,
Beobachtungen und Befragungen zulässige Datenquellen\footcite[Vgl. ][]{(Benbasatet
  al. 1987; Eisenhardt1989). (dubeRigorIn-formationSys-tems2003 S. 612),
  Yin(1994)}. Auch sämtliche andere Artefakte der Fälle, sowohl in
physischer als auch in digitaler Form können zu der Untersuchung
hinzugezogen werden\footcite[Vgl. ][]{a.
  (gothlichFallstudienAlsForschungsmethode2003 S.10);
  (dubeRigorInformationSystems2003 S. 612); Yin(1994)}.

Zu diesen Primärdaten könnten auch noch Sekundärdaten, wie z.B.
Interpretationen, Zusammenfassungen, Textanalysen, Statistiken und
Berichte hinzugezogen werden
(gothlichFallstudienAlsForschungsmethode2003 S. 11).

Für eine möglichst valide Endaussage sei eine Kombination der Quellen
besonders sinnvoll (gothlichFallstudienAlsForschungsmethode2003 S. 10).
Diese Verwendung mehrerer Quellen und auch mehrerer Forschungsmethoden
kann als methodologische Triangulation verstanden werden
(gothlichFallstudienAlsForschungsmethode2003 S. 10).

Bei allen Quellen sei es besonders wichtig, Daten ähnlich wie nach den
„Grundsätzen ordnungsgemäßer Buchführung`` in einer geordneten Form
lückenlos und unverfälscht aufzubewahren, sodass eine spätere Prüfung
der Schlussfolgerungen möglich sei
(gothlichFallstudienAlsForschungsmethode2003 S. 11).

Im Sinne dieser Empfehlungen zur Auswahl der Quellen wird auch die
Auswahl der Quellen in dieser Arbeit geplant. Es werden zwei
Primärquellen berücksichtigt. Zum einen werden Dokumente aus der
Projektmanagementsoftware der Projekte bezogen. Zum anderen werden
Quelltextartifakte aus der Versionsverwaltung analysiert. Zusätzlich zu
diesen Primärquellen wird in jedem Projekt ein Abschlussinterview mit
einer strukturierten Befragung durchgeführt. Diese Abschlussinterviews
sollen die Interpretation der Primärdaten stützten und die
ordnungsgemäße Sammlung dieser validieren.

\section{Verbindung von Daten und Hypothesen}\label{verbindung-von-daten-und-hypothesen}

Die eigentliche Beantwortung der Problemstellung bedarf einer
Ausrichtung der Daten der Analyseeinheiten (Punkt 3 Units of analysis)
als Reflektion der initialen Hypothese (yinCaseStudyResearch2014 S. 78,
gothlichFallstudienAlsForschungsmethode2003 S. 11). Diese Reflektion
bzw. Verbindung kann mit verschiedenen Analyseverfahren hergestellt
werden. Dabei stehen unter anderem Mustervergleiche, Erklärungsgebilde,
Zeitserienanalysen, logische Modelle und fallübergreifende Synthesen zur
Verfügung (yinCaseStudyResearch2014 S. 78,
gothlichFallstudienAlsForschungsmethode2003 S. 11). Wenn bekannt ist,
dass einige oder alle Hypothesen eine Entwicklung über Zeit betrachten,
könne zum Beispiel eine Zeitserienbetrachtung sinnvoll sein
(yinCaseStudyResearch2014 S. 78, 225).

In dieser Arbeit wird die Komplexität der Software in Projekten
betrachtet (siehe Unit of Analysis). Dabei sollen die
Komplexitätsmetriken mit Aufwandsabschätzungen verglichen werden. Dieser
Vergleich bzw. diese Verbindung ist in Abbildung .. dargestellt und wird
im Folgenden erläutert.

GRAFIK

Für jedes Softwareprojekt sind Daten zur Codekomplexität und zu den
Aufwandsabschätzungen vorhanden.

Die Codekomplexität kann aus dem Sourcecode der Software berechnet
werden (siehe Analysesoftware). Der Sourcecode aller Projekte wird
jeweils in einer Versionsverwaltungssoftware verwaltet. Mit einer
solchen Versionsverwaltungssoftware lassen sich jegliche Änderungen am
Code historisch nachvollziehen. Gleichzeitig kann so auch für jeden
Änderungszeitpunkt in retrospektive der Sourcecode rekonstruiert
werden\footcite[Vgl. ][]{Quelle Git}. Also lässt sich über die komplette
Entwicklungsgeschichte der Software hinweg rekonstruieren, wie der
Sourcecode zu dem jeweiligen Zeitpunkt ausgesehen haben muss. Aus der
Historie des Sourcecodes lässt sich auch eine Historie der
Sourcecodekomplexität berechnen. Dabei wird zu jedem Zeitpunkt in
Retrospektive der Sourcecode hinsichtlich seiner Komplexität analysiert.
Auf diese Weise lässt sich eine Zeitserie der Komplexität des
Sourcecodes über die Entwicklungsgeschichte hinweg rekonstruieren. Auf
der linken Seite von Abbildung .. ist beispielhaft eine solche Serie
über drei Zeitpunkte hinweg skizziert. In realen Projekten lassen sich
zwischen 1000 und 50.000 distinktive Zeitpunkte rekonstruieren. Die
Abbildung hier dient beispielhaft dazu, den Anstieg der Komplexität der
Software zu veranschaulichen.

Als zweite Vergleichsgröße sollen Aufwandsabschätzungen dienen. Auch
hier lässt sich eine Zeitserie aufbauen. Alle Projekte werden nach der
agilen Scrum Methodik verwaltet (Verweis zu Kapitel 3). Diese Methodik
sieht vor, im Voraus zu der Implementierung eines Softwarefeatures den
Umfang bzw. Implementierungsaufwand dieses abzuschätzen. Diese
Abschätzungen werden numerisch in Form von sog. Storypoints abgegeben
und in einer Projektmanagementsoftware (wie z.B. Jira vermerkt). Auch
wird zusammen mit der Aufwandsabschätzung unter anderem der Anfangs- und
der Endzeitpunkt der Implementierung vermerkt. Diese Daten sind bei
allen betrachteten Projekten über die komplette Entwicklungsdauer hinweg
verfügbar. Es lässt sich also rekonstruieren, zu welchem Zeitpunkt
welcher geschätzte Aufwand in die Software geflossen ist. Die
Kombination der Komplexitätsabschätzung zusammen mit ihrem Zeitpunkt
ergibt ebenfalls eine Zeitreihe. Diese ist in Abbildung .. beispielhaft
auf der linken Seite skizziert. Die Entwicklung von Features in einer
Software erfolgt konsekutiv und konstruktiv. Das heißt, wird an
Zeitpunkt 1 Feature 1 und an Zeitpunkt 2 Feature 2 entwickelt, besteht
die Software zu Zeitpunkt 2 aus den Features 1 und 2. Dieser
Sachzusammenhang wird ebenfalls in Abbildung .. skizziert. Der
insgesamte, abgeschätzte Funktionsumfang der Software zu einem Zeitpunkt
X besteht also aus allen Features, die zum Zeitpunkt X entwickelt
wurden, addiert zu den summierten Features, welche bereits vor Zeitpunkt
X entwickelt wurden. Die Aufwandsabschätzungen der Features (Stories)
sind zu den distinktiven Zeitpunkten in Form von Storypoints verfügbar.
Also lässt sich durch die Kumulierung aller Aufwandsabschätzungen
(Storypoints) bis zu diesem Zeitpunkt der abgeschätzte Funktionsumfang
der Software zu diesem Zeitpunkt ableiten. Dieser Sachzusammenhang ist
auf der rechten Seite von Abbildung .. dargestellt.

Zusammengefasst konnten bis hierhin zwei Zeitreihen konstruiert werden.
Einmal auf der linken Seite von Abbildung .. die berechnete
Codekomplexität der Software und dann auf der rechten Seite die
abgeschätzte Komplexität der Software. Beide Werte liegen zeitdistinktiv
zu einer Vielzahl von Zeitpunkten vor. Betrachtet man nun einen
einzelnen Zeitpunkt, sollte laut der anfänglichen Hypothese der
abgeschätzte Gesamtaufwand der Software der berechneten Codekomplexität
entsprechen. Zur Validierung der Hypothese lässt sich eine Korrelation
mathematisch berechnen. Somit ist über die Korrelation der beiden
Zeitreihen eine Verbindung der Fälle (Units of Analysis) über die
gesammelten Daten mit der anfänglichen Hypothese hergestellt.

\section{Kriterien zur Interpretation der Daten}\label{kriterien-zur-interpretation-der-daten}

Die in 4. erläuterte Korrelation der Aufwandsabschätzungen mit den
Codekomplexitätsmetriken soll in einem finalen Schritt interpretiert
werden. Im Sinne einer typischen Fallstudienforschung soll dabei eine
Erklärung für die aufgetretenen Phänomene gefunden werden
(gothlichFallstudienAlsForschungsmethode2003 S. 11?).

In der einschlägigen Literatur über die Durchführung von Fallstudien
finden sich verschiedene Techniken zur Interpretation der gesammelten
Daten. Darunter sind unter anderen Techniken der statistischen Analyse
(yinCaseStudyResearch2014 S. 79,
gothlichFallstudienAlsForschungsmethode2003 S. 11??) sowie Interviews
(gothlichFallstudienAlsForschungsmethode2003 S. 11?). Mit diesen beiden
Techniken werden die Analyseergebnisse aus 4.2.4 zunächst interpretiert
und dann validiert.

Im Sinne einer objektiven Vergleichbarkeit erscheint eine mathematische
Interpretation der Daten mit Methoden der Statistik sinnvoll. Auch in
verwandten Arbeiten wird mit mathematischen Verfahren gearbeitet. Diese
Arbeit strebt einen Nachweis einer Korrelation zwischen zwei Zeitreihen
an. Demnach erscheint es sinnvoll, Korrelationsmaße für den Vergleich
von Zeitreihen anzuwenden. In verwandten Arbeiten kommen unter anderem
die Pearson-Produkt-Moment-Korrelation, der Kendall
Rangkorrelationskoeffizient sowie der Spearman
Rangkorrelationskoeffizient zum Einsatz\footcite[Vgl. ][]{(Jay et al 2009:140),
  hindleReadingLinesUsing2009}.

Die Pearson-Produkt-Moment Korrelation ist ein parametrisches Verfahren
zur Berechnung des bivariaten Zusammenhangs zwischen zwei Größen. Der
Korrelationskoeffizient kann Werte zwischen -1 und 1 annehmen. Bei einem
Wert von +1 besteht ein vollständig positiver linearer Zusammenhang. Bei
einem Wert von -1 ein vollständig negativer. Das Korrelationsmaß wurde
erstmals von Sir Francis Galton verwendet und von Karl Pearson zuerst
formal-mathematisch begründet\footcite[Vgl. ][]{brucklerGeschichteMathematikKompakt2018
S. 116}.

Die Ergebnisse können über ihren Betrag grob eingeordnet
werden\footcite[Vgl. ][]{fahrmeirStatistikWegZur2016 S. 130}:

\begin{itemize}
\item
    „schwache Korrelation`` r <= 0.5
\item
    „mittlere Korrelation`` 0.5 <= r <= 0.8
\item
    „starke Korrelation`` 0.8 <= r.
\end{itemize}

Ein weiterer Korrelationskoeffizient ist der Kendall
Rangkorrelationskoeffizient. Zur Berechnung des Koeffizienten der die
Ordnungsrelationen aller möglicher Paare der beobachteten Merkmalswerte
für die zwei Merkmale verglichen. Im Vergleich zu Pearson ist dieser
Koeffizient robuster gegenüber Ausreißern und unabhängig von der
metrischen Skalierung der Daten\footcite[Vgl. ][]{fahrmeirStatistikWegZur2016 S.
  137ff}. Die Ergebnisse sind jedoch gleich zu interpretieren.

Ein dritter Korrelationskoeffizient ist der Spearman
Rangkorrelationskoeffizient. Dieser wird ähnlich wie der
Rangkorrelationskoeffizient nach Kendal berechnet, zieht jedoch
zusätzlich zu den Unterschieden zwischen den Rängen noch die Differenz
zwischen den Rängen hinzu\footcite[Vgl. ][]{fahrmeirStatistikWegZur2016 S. 133f}.
Auch hier sind die Ergebnisse gleich wie bei den anderen beiden
Koeffizienten zu interpretieren.

Zur Validierung dieser Interpretation wird zusätzlich in jedem der
Projekte ein Interview mit den Key-Stakeholdern durchgeführt. Das
Interview verfolgt dabei drei Ziele. Zunächst werden generelle
Informationen zu dem Projekt aufgenommen. Dann werden die Stakeholder
befragt, ob ihnen offensichtliche Messfehler in den Projekten auffallen.
In einem letzten Schritt werden in einer offenen Diskussion interessante
Punkte in den Arbeitsergebnissen identifiziert. Diese Interessenspunkte
werden dann versucht zu erklären.

Während bei vielen Forschungsmethoden die genaue Transkription und
Analyse von wenig strukturierten Interviews (z.B. durch die
Inhaltsanalyse nach Mayring) im Fokus steht, wird bei dieser Arbeit ein
strukturiertes Vorgehen bei der Durchführung angestrebt, um die
Interviews so durch logische Schlüsse analysieren zu können. Dieses
Vorgehen wird auch von verbreiteter Literatur zum generellen Vorgehen
bei der Fallstudienforschung unterstützt
(gothlichFallstudienAlsForschungsmethode2003 S. 12). Dabei birge eine zu
detailreiche Analyse der Interviews die Gefahr der Überinterpretation.
Wichtiger sei es, die Ergebnisse der Interviews kritisch zu reflektieren
(gothlichFallstudienAlsForschungsmethode2003 S. 12). Für ein möglichst
strukturiertes Vorgehen in den Interviews wurde vorab mit mehreren
Experten aus dem Feld der Softwareentwicklung ein Leitfaden in Form
eines Fragebogens erstellt. Dieser kann Anhang .. entnommen werden.

Die so erlangten Ergebnisse der einzelnen Fälle sollen auch über die
Fälle hinweg verglichen und interpretiert werden. Kommen bei einer
Fallstudie mehrere Fälle zu dem gleichen Ergebnis spricht man von einer
Replikation (gothlichFallstudienAlsForschungsmethode2003 S. 11). Wenn
bei allen Fällen eine Korrelation von Aufwandsabschätzungen und
Komplexitätsmaßen festgestellt werden kann, ließe dies auf eine
allgemeine Regel schließen. Aber auch eine sog. theoretische Replikation
ist vorstellbar. Dabei kommen verschiedene Fallstudien zu verschiedenen
Ergebnissen, welche aber theorieseitig erklärbar sind
(gothlichFallstudienAlsForschungsmethode2003 S.11).

\section{Implementierung einer
Untersuchungssoftware}\label{implementierung-einer-untersuchungssoftware}

Für die Untersuchung der Softwarekomplexitätsmetriken müssen einige
komplexe Berechnungen durchgeführt werden. Die Datenbasis dieser
Untersuchung umfasst mehrere zehntausend Datenpunkte. Eine manuelle
Verarbeitung dieser Daten erscheint als wenig ökonomisch. Also bedarf es
einer automatisierten Verarbeitung der Daten. Die Verarbeitung der Daten
soll mit einer Software erfolgen. In einer umfangreichen Suche konnte
lediglich die Software SonarQube einen wesentlichen Anteil der
Anforderungen an ein solches Produkt abdecken. Bei dem Ansetzen einer
ersten SonarQube Instanz stellte sich jedoch heraus, dass die Import-
und Exportfunktionen von SonarQube unzureichend für die hier angestrebte
Untersuchung sind. Da außer SonarQube keine passende Software gefunden
werden konnte, liegt es auf der Hand, eine eigene Untersuchungssoftware
zu implementieren.

\subsection{Anforderungen an die Untersuchungssoftware}\label{Anforderungen-an-die-Untersuchungssoftware}

Die Berechnung der Korrelation von Codekomplexitätsmetriken und
Komplexitätsabschätzungen stellt eine Reihe an Anforderungen an die
Untersuchungssoftware. Zunächst werden die Anforderungen in einem
Überblick zusammengefasst und dann im späteren Verlauf dieses Kapitels
genauer betrachtet.

Als Eingabe werden zwei Datenströme vorgesehen. Auf der einen Seite die
Entwicklungshistorie der Software und auf der anderen Seite die
historischen Komplexitätsabschätzungen für die Features der Software.
Besonders zu bemerken ist, dass beide Datenströme irreguläre
Zeitintervalle und unterschiedliche Wertebereiche haben.

Funktionale Anforderungen:

\begin{itemize}
\item
    Verarbeitung passender Eingabedatenströme

    \begin{itemize}
    \item
    Als ersten Eingabeparameter sind die Komplexitätsabschätzungen
    vorgesehen. Wie in 4.2.4 beschrieben, liegen bei allen behandelten
    Projekten für jedes entwickelte Feature Komplexitätsabschätzungen in
    einer Projektmanagementsoftware vor. Aus dieser Software können die
    Schätzungen in Tabellenform als CSV-Datei exportiert werden. Der
    Export soll unverarbeitet als ersten Eingabeparameter in die
    Untersuchungssoftware eingelesen werden. Dabei muss diese Tabelle
    zwei Spalten beinhalten. Ein Datensatz in dieser Tabelle muss
    jeweils einen Zeitstempel mit dem Entwicklungszeitpunkt des
    Features, sowie die Komplexitätsabschätzung als Anzahl von
    Storypoints beinhalten.
    \item
    Als zweiten Eingabeparameter soll die Versionshistorie der Software
    dienen. Bei allen zu untersuchenden Projekten liegt die
    Versionshistorie in Form eines sog. Repositories der
    Versionsverwaltungssoftware Git\footnote{Git erklären} vor. Diese
    Versionshistoriendaten sollen von der Untersuchungssoftware
    verarbeitet werden können.
    \end{itemize}
\item
    Generierung zielgerichteter Ausgangsdatenströme

    \begin{itemize}
    \item
    Zunächst wird als erster Ausgangsdatenstrom vorgesehen, dass in
    einem regulären Zeitintervall von einer Stunde über die komplette
    Entwicklungshistorie der Software hinweg jeweils die kumulierten
    Storypoints, sowie der jeweilige Stand der Komplexitätsmetriken
    vorliegen. Ist zu dem spezifischen Zeitpunkt in dem Zeitintervall
    kein Wert vorhanden, soll dieser aus den Nachbarwerten linear
    interpoliert werden. Als Ausgabeformat wird eine Tabelle in Form
    einer CSV Datei vorgesehen:
    \end{itemize}
\end{itemize}

TODO!TABLE

\begin{itemize}
\item
    Als zweiter Ausgabeparameter wird ein Graph des Verlaufes aller
    Maßzahlen im PNG- und PDF-Format vorgesehen.
\item
    In einem dritten Ausgabeparameter soll die Korrelation der Maßzahlen
    mit den Storypoints mit verschiedenen Korrelationsmaßen dargestellt
    werden. Als Korrelationsmaße ist die
    Pearson-Produkt-Moment-Korrelation, die Kendall Rangkorrelation und
    die Spearman Rangkorrelation vorgesehen. In dem dritten
    Ausgabeparameter sollen diese Koeffizienten als Tabelle im CSV-Format
    für jedes Komplexitätsmaß angegeben werden.
\item
    Als vierten Ausgabeparameter ist eine Heatmap\footnote{Definition
    Heatmap} mit den Korrelationskoeffizienten auf der x-Achse und den
    Komplexitätsmetriken auf der y-Achse vorgesehen.
\end{itemize}

\begin{itemize}
\item
    Für eine möglichst breite Anwendbarkeit soll die Software unter
    Unix-basierten Betriebssystemen, wie Linux und Mac ausführbar sein.
    Außerdem können so eine Vielzahl von Open-Source Bibliotheken
    angebunden werden.
\end{itemize}

Nicht-funktionale Anforderungen

\begin{itemize}
\item
    Der Fokus dieser Arbeit liegt nicht auf der Implementierung der
    Untersuchungssoftware, sondern auf den mit ihr erzielten Ergebnissen.
    Für eine möglichst ökonomische Beantwortung der Forschungsfragen
    erscheint es also von Interesse, die Untersuchungssoftware mit einem
    möglichst geringen Implementierungsaufwand umzusetzen.
\item
    In dieser Arbeit sollen einer Vielzahl von teils sehr umfangreichen
    Projekten analysiert werden. Um eine zeitgerechte Abwicklung der
    Analysen zu gewährleisten, soll der Untersuchungssoftware möglichst
    effizient mit Rechenressourcen umgehen.

    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
    \item
    \protect\hypertarget{_Toc102600182}{}{}Aufbau der
    Untersuchungssoftware
    \end{enumerate}
\end{itemize}

Zur Begegnung der zuvor genannten Anforderungen wird eine
Untersuchungssoftware skizziert. Die Transformation der Eingabedaten zu
den Ausgabedaten bedarf einer Vielzahl von komplexen Operationen. Für
eine ökonomische Umsetzung der Software (siehe Anforderung ..) wird
angestrebt, einen möglichst großen Anteil des Funktionsumfangs der
Software durch eine Wiederverwendung von bereits vorhandenen
Softwaremodulen zu realisieren. Im Zuge dessen wird auch eine
Implementierung über mehrere Programmiersprachen hinweg in Kauf
genommen. Eine Skizze der so erreichten Implementierung wird in
Abbildung TODO! dargestellt und im Folgenden erklärt.

TODO!GRAFIK

Der Untersuchungssoftware ist in drei Teile unterteilt. Zunächst liest
der Code Parser (Zahl) die Versionshistorie der Software ein und
berechnet für jeden Änderungsstand (Commit) die Codekomplexitätsmetriken
der Software. Die so berechneten Metriken werden in einem zweiten Modul
(2) mit den Daten aus dem Projektmanagement Tool zusammengefügt,
verarbeitet und zu den Ausgabedaten aufbereitet.

\textbf{Der Code Parser}

Der Code Parser ist dafür verantwortlich, die einzelnen
Entwicklungsstände der Software aus der Versionshistorie zu extrahieren
und für jeden Entwicklungsstand die Komplexitätsmetriken zu berechnen.
Als Eingabeparameter ist ein Repository der Versionsverwaltungssoftware
Git vorgesehen. Als Ausgabeparameter sind die
Softwarekomplexitätsmetriken in Tabellenform als CSV Datei vorgesehen
(Tabelle ..). Der Code Parser besteht aus einigen generellen
Kontrollstrukturen und speziellen Analysemodulen, welche die
Komplexitätsmetriken für verschiedene Programmiersprachen in den
Projekten berechnen. Für die Analysemodule wird auf bereits verbreitete
Bibliotheken zurückgegriffen, da eine Eigenentwicklung der Module zu
aufwändig wäre. Bei der Auswahl der Bibliotheken wurde darauf geachtet,
möglichst gut gewartete Software zu verwenden, um so ein möglichst
genaues Ergebnis zu erzielen. Eine Tabelle mit allen betrachteten
Bibliotheken findet sich in Anhang .. . Für ein noch genaueres Ergebnis
werden manche Metriken doppelt berechnet. In dem
Zahlenverarbeitungsmodul wird später das jeweils genauste Ergebnis
ausgewählt. Der Ablauf des Code Parsers wird im folgenden erläutert:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
    \textbf{Commit Getter}: Zunächst werden durch den Commit Getter alle
    Entwicklungsstände der Software mit ihrem jeweiligen Commit-Hash und
    dem Zeitstempel des Eincheckzeitpunktes in eine CSV Datei geschrieben
\item
    \textbf{Commit Runner:} Der Commit Runner liest diese CSV-Datei,
    iteriert über alle Commits und führt für jeden Commit der Software
    alle Analysemodule aus. Die Analysemodule schreiben jeweils die
    Ergebnisse der Analyse des jeweiligen Commits in eine CSV-Datei mit
    dem Commit-Hash als Dateinamen.

    \begin{enumerate}
    \def\labelenumii{\alph{enumii}.}
    \item
    \textbf{Lizard}: Das Python Modul Lizard berechnet für alle
    Programmiersprachen die zyklomatische Komplexität\footcite[Vgl. ][]{LizardPythonModule}.
    \item
    \textbf{Plato}: Das NodeJS Commandline-Tool ES6-Plato\footcite[Vgl. ][]{Es6platoLibMaster}
    kann lediglich JavaScript Quelltexte analysieren. Für die JavaScript
    Teile des zu untersuchenden Projektes berechnet Plato noch einmal
    die zyklomatische Komplexität, den Halstead Aufwand und die Anzahl
    logischer Codezeilen
    \item
    \textbf{Indentation Complexity}: Mit dem Rust Commandline-Tool
    Complexity der Firma thoughtbot, inc\footcite[Vgl. ][]{Complexity2022} wird
    die Einrückungskomplexität für alle Programmiersprachen berechnet
    \item
    \textbf{MulitMetric}: Das MultiMetric Python Modul\footcite[Vgl. ][]{weihmanMultimetric2021}
    ist zuletzt in der Lage für sämtliche hier behandelte
    Programmiersprachen die zyklomatische Komplexität, den Halstead
    Aufwand und die Anzahl logischer Codezeilen zu berechnen.
    \end{enumerate}
\item
    \textbf{Consolidator:} Als letzter Schritt werden im Consolidator die
    Ergebnisse aller Analysemodule für alle Entwicklungsstände in einer
    Tabelle konsolidiert und als eine CSV-Datei ausgegeben. Dafür liest
    der Consolidator zunächst die Commit-Hashes und Zeitstempel aus der,
    von dem Commit-Getter generierten CSV Datei. Für jeden Commit wird für
    jedes Analysemodul nach einer Ergebnis-CSV-Datei gesucht. Für jede
    Komplexitätsmetrik wird eine Summe\footcite[Vgl. ][]{(Hoffmann 2013:276)} aller
    Dateien in dem Analyseergebnis gebildet. Die Summen der
    Komplexitätsmetriken der einzelnen Entwicklungsstände werden in einer
    CSV-Datei ausgegeben.
\end{enumerate}

Die Ausgabe des Consolidators ist gleichzeitig auch die Ausgabe des Code
Parsers. Als solche wird sie in Tabelle .. beschrieben.

TODO!TABELLE

\subsection{Zahlenverarbeitungsmodul}\label{Zahlenverarbeitungsmodul}

Nachdem in dem Code Parser die Komplexitätsberechnungen stattgefunden
haben, sollen diese nun in dem Zahlenverarbeitungsmodul zusammen mit den
Aufwandsabschätzungen zu dem Analyseergebnis verarbeitet werden.

Das Zahlenverarbeitungsmodul verarbeitet zwei Eingabeparameter. Zunächst
wird das Resultat des Code-Parser-Moduls (siehe Tabelle ..) gelesen. Wie
in 4.2.4 beschrieben ist als zweiter Eingabeparameter der Export von
einer Projektmanagementsoftware vorgesehen. In diesem Export sollte eine
CSV-Tabelle mit Datensätzen von den Features der Software enthalten
sein. Jeder Datensatz beinhaltet mindestens zwei Datenpunkte, einen
Zeitstempel (date) und eine Komplexitätsabschätzung (storypoints). Eine
Beispiel-Eingabetabelle kann Tabelle .. entnommen werden.

TODO!TABELLE

Die Operationen innerhalb dieses Moduls befassen sich primär mit der
Analyse und Verarbeitung großer Datenmengen. Das Wissenschaftsfeld Data
Science behandelt ebenfalls genau diese Operationen. In Data Science ist
die Programmiersprache Python weit verbreitet\footcite[Vgl. ][]{Quelle fehlt}.
Also liegt es auf der Hand, auch für dieses Modul die Programmiersprache
Python zu verwenden. Für Python existiert eine Vielzahl von Bibliotheken
zur Verarbeitung von großen Datenmengen. Für dieses Modul wird primär
die Datenanalyse- und Manipulationsbibliothek Pandas
verwendet\footcite[Vgl. ][]{\url{https://pandas.pydata.org/}}. Zusätzlich wird
die Bibliothek Matplotlib zum Erstellen von Graphen zur Hilfe gezogen.
In der Analyse der Daten erscheint es zusätzlich sinnvoll,
Zwischenergebnisse interaktiv zu verifizieren. So können Analysefehler
bereits früh erkannt und behoben werden. Die Software Jupyter ermöglicht
genau solch eine interaktive Programmausführung und wird somit zur
Ausführung des Moduls herangezogen.

Der Aufbau und die Implementierung des Zahlenverarbeitungsmoduls werden
nun erläutert. Das Modul lässt sich in vier Teile aufteilen:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
    Verarbeiten der Codekomplexitätsmessungen zu einer Zeitreihe
\item
    Verarbeiten der Aufwandsabschätzungen zu einer Zeitreihe
\item
    Zusammenführen beider Zeitreihen
\item
    Analysieren der Zeitreihen
\end{enumerate}

Im ersten Schritt zur Verarbeitung der Codekomplexitätsmessungen werden
diese zunächst als Pandas Dataframe in das Modul geladen (Zeile 1). Bis
auf den Zeitstempel können alle Datentypen automatisch erkannt werden.
Der Datentyp des Zeitstempels wird manuell gesetzt (Zeile 2)

\lstdefinestyle{pythonStyle}{
%  basicstyle=\fontsize{7}{8}\selectfont\ttfamily
  language=Python,
  numbers=left,
  stepnumber=1,
  numbersep=10pt,
  breaklines=true,
  tabsize=4,
  showspaces=false,
  showstringspaces=false
}

\lstset{style=pythonStyle}
\lstinputlisting{includes/4 research setup/code-analyser-cells/cell-4.py}

Dann wird der Zeitstempel als Zeitindex gesetzt (Zelle 5 Zeile 1-3)

\lstset{style=pythonStyle}
\lstinputlisting{includes/4 research setup/code-analyser-cells/cell-5.py}

Als zweiter Schritt ist eine Untersuchung der gelesenen Daten
vorgesehen. Dabei wird zunächst untersucht, welche Ergebnisse in der
Eingabe vorhanden sind (Zelle 7 Zeile 1-4).

\lstset{style=pythonStyle}
\lstinputlisting{includes/4 research setup/code-analyser-cells/cell-7.py}

Weiter wird eine Übersicht erstellt, zu welchen dieser Ergebnisse valide
Werte vorliegen (Zelle 8 Zeile 1-17). Dabei werden alle Ergebnisse
aussortiert, bei denen mehr als zehn Prozent der Messwerte ungültig
sind.

\lstset{style=pythonStyle}
\lstinputlisting{includes/4 research setup/code-analyser-cells/cell-8.py}

Nun wird für jede zu berechnende Komplexitätsmetrik (Anzahl logischer
Codezeilen, zyklomatische Komplexität, Halstead Aufwand und
Einrückungskomplexität) das beste Ergebnis aus den vorliegenden
Ergebnissen der Analysemodule ausgewählt (Zelle 9). Für diese Auswahl
wurde zuvor in dem Zahlenverarbeitungsmodul ein Zuweisungsobjekt
hinterlegt (Zelle 2). In diesem Zuweisungsobjekt wird für jede Maßzahl
eine Präferenzliste der möglichen Ergebnisse der Analysemodule
hinterlegt. In dieser Zelle wird nun über die zu berechnenden
Komplexitätsmetriken iteriert und für jede Metrik das erste valide
Analyseergebnis aus der Präferenzenliste ausgewählt. Die Endauswahl wird
in einer Variable abgespeichert.

\lstset{style=pythonStyle}
\lstinputlisting{includes/4 research setup/code-analyser-cells/cell-9.py}

Anhand dieser Zuweisungstabelle wird nun ein Dataframe mit den besten
Analyseergebnissen für jede Metrik aufgebaut. Für alle Datenpunkte, bei
denen an dieser Stelle noch kein Wert vorhanden ist, muss davon
ausgegangen werden, dass kein Wert ermittelt werden konnte. Diese werden
mit 0 aufgefüllt (Zelle 10, Zeile 6)

\lstset{style=pythonStyle}
\lstinputlisting{includes/4 research setup/code-analyser-cells/cell-10.py}

Bis zu diesem Punkt konnte nun eine Tabelle mit den Datensätzen der
Komplexitätsmessungen aufgebaut werden. Ein Beispiel ist in Tabelle ..
aufgeführt. Jeder Datensatz beinhaltet einmal den Zeitstempel der
Komplexitätsmessung (timestamp), die Anzahl logischer Codezeilen
(logicalLinesOfCode), die zyklomatische Komplexität
(cyclomaticComplexity), den Aufwand nach Halstead (halsteadEffort) und
die Einrückungskomplexität (indendationComplexity).

TODO!TABELLE

Als nächster Schritt werden die Daten in einen einheitlichen
Wertebereich normalisiert (Zelle 11). Dabei werden die Daten so linear
verschoben und skaliert, dass jeweils der erste Wert jeder Zeitreihe 0
ist und der letzte Wert 1 ist.

\lstset{style=pythonStyle}
\lstinputlisting{includes/4 research setup/code-analyser-cells/cell-11.py}

Zusätzlich soll nun auch das Zeitintervall normalisiert bzw.
regularisiert werden. Dabei werden die Daten zunächst auf ein geringes
Intervall von 60 Sekunden abgetastet. Die fehlenden Werte werden linear
interpoliert. In einem durchschnittlichen Projekt werden so ca. 2
Millionen Datensätze generiert. Diese Anzahl von Datensätzen ist jedoch
zu groß, um sie weiter zu verarbeiten. Für die weitere Verarbeitung wird
die Zeitreihe erneut auf ein Intervall von einer Stunde abgetastet, was
die Anzahl an Datensätzen auf ca. 34 tausend reduziert.

\lstset{style=pythonStyle}
\lstinputlisting{includes/4 research setup/code-analyser-cells/cell-12.py}

Bis zu diesem Punkt wurde für jede Komplexitätsmetrik eine passende
Messreihe ausgewählt und die Werte auf ein einheitliches Zeitintervall,
sowie einen einheitlichen Wertebereich normalisiert. Diese Daten können
in dieser Form zusammen mit den Aufwandsabschätzungen weiterverarbeitet
werden.

Als zweiter Arbeitsschritt befasst sich das Zahlenverarbeitungsmodul mit
der Verarbeitung der Aufwandsabschätzungen. Diese Daten werden ebenfalls
zunächst in einen Pandas Dataframe geladen und bereinigt (Zelle 13).

\lstset{style=pythonStyle}
\lstinputlisting{includes/4 research setup/code-analyser-cells/cell-13.py}

Weiter werden die Aufwandsabschätzungen nach ihrem Zeitpunkt aufsteigend
sortiert (Zelle 14).

\lstset{style=pythonStyle}
\lstinputlisting{includes/4 research setup/code-analyser-cells/cell-14.py}

Weiter werden die relevanten Spalten aus den Daten ausgewählt (Zeile 1),
der Index als Zeitstempel in der UTC Zeitzone gesetzt (Zeile 2 und 3),
die Spalten mit einheitlichen Namen versehen und dann das resultierende
Datenset noch einmal nach Datum aufsteigend sortiert (Zeile 5).

\lstset{style=pythonStyle}
\lstinputlisting{includes/4 research setup/code-analyser-cells/cell-16.py}

Nun werden die Storypoints wie in 4.2.4 beschrieben kumuliert (Zelle
15). Hierzu bietet Pandas die Funktion cumsum an\footcite[Vgl. ][]{PandasDataFrameCumsum}.

\lstset{style=pythonStyle}
\lstinputlisting{includes/4 research setup/code-analyser-cells/cell-17.py}

Dann wird in dem gleichen Verfahren wie bei den Codekomplexitätsmetriken
der Wertebereich der Aufwandsabschätzungen auf einen Bereich zwischen 0
und 1 normalisiert (Zelle 16).

\lstset{style=pythonStyle}
\lstinputlisting{includes/4 research setup/code-analyser-cells/cell-18.py}

Auch das Zeitintervall der Messdaten wird zu einem regulären
Zeitintervall von einer Stunde vereinheitlicht (Zelle 17).

\lstset{style=pythonStyle}
\lstinputlisting{includes/4 research setup/code-analyser-cells/cell-19.py}

Das resultierende Datenset hat nun also genau wie die Codemetriken einen
Wertebereich zwischen null und eins und eine Abtastrate von 60 Sekunden.
Die beiden Zeitreihen sind nun also sowohl in ihrer Skalierung als auch
in ihrer Abtastrate identisch.

Als dritter Schritt des Zahlenverarbeitungsmoduls können die Zeitreihen
der Codekomplexitätsmessungen und der Aufwandsabschätzungen nun
zusammengeführt werden. Das geschieht mit der Merge-Funktion von
Pandas\footcite[Vgl. ][]{PandasDataFrameMergea}. Die Merge-Funktion stellt eine
neue Tabelle mit den Werten der Codekomplexitätsmessungen und der
Aufwandsabschätzungen auf. Durch den „how=inner`` Parameter wird
festgelegt, dass ein Inner-Merge durchgeführt wird. Bei einem
Inner-Merge werden nur Zeitpunkte berücksichtigt, an denen in beiden
Zeitreihen ein Wert verhanden ist.

\lstset{style=pythonStyle}
\lstinputlisting{includes/4 research setup/code-analyser-cells/cell-21.py}

Die zusammengefügten Daten werden nun noch einmal zusammen normalisiert
(Zelle 22)

\lstset{style=pythonStyle}
\lstinputlisting{includes/4 research setup/code-analyser-cells/cell-22.py}

Die normalisierten Daten werden als erster Ausgabeparameter der
Analysesoftware (siehe 4.7.1) als CSV-Datei exportiert.

Weiter sollen die Daten nun analysiert werden. Dazu wird zunächst mit
der Pandas Mean-Funktion\footcite[Vgl. ][]{PandasDataFrameMean} eine Zeitreihe
des arithmetischen Mittels aller Komplexitätsmetriken berechnet (Zelle
24).

\lstset{style=pythonStyle}
\lstinputlisting{includes/4 research setup/code-analyser-cells/cell-24.py}

Nun wird die Differenz dieses arithmetischen Mittels und den
Aufwandsabschätzungen gebildet (Zelle 25). Ist diese Differenz besonders
groß, heißt das, dass die Komplexitätsmessungen an dieser Stelle
besonders stark von den Aufwandsabschätzungen abweichen.

\lstset{style=pythonStyle}
\lstinputlisting{includes/4 research setup/code-analyser-cells/cell-25.py}

Aus den bis jetzt berechneten Daten kann der Korrelationsgraph
aufgestellt werden. Dieser wird als zweiter Ausgabeparameter (siehe 4.7.1) in Form von einer
PDF und einer PNG Datei exportiert. Der Code zur Generierung des Korrelationsgraphen findet sich im Anhang TODO.

Als letzter Schritt können die Korrelationen der Komplexitätsmetriken
mit den Aufwandsabschätzungen berechnet werden (Zelle 23). Das ist in
dem Pandas Modul mit der Corr-Funktion\footcite[Vgl. ][]{PandasDataFrameCorr}
möglich. Die einzelnen Korrelationskoeffizienten werden dann zu einer
Tabelle zusammengefügt (Zeile 13) und als CSV-Datei exportiert. Diese
CSV-Datei stellt den dritten Ausgabeparameter der Analysesoftware dar
(siehe 4.7.1).

Zur Visualisierung der Korrelationskoeffizienten wird zuletzt noch eine
Heatmap als vierter und letzter Ausgabeparameter (siehe 4.7.1) erstellt
und im PNG und PDF Format exportiert.

Zusammengefasst konnten mit der hier erläuterten Software die
Komplexitäten zu den verschiedenen Entwicklungszeitpunkten der Software
berechnet werden und zusammen mit den Aufwandsabschätzungen analysiert
werden.

\section{Grenzen und Probleme}\label{grenzen-und-probleme}

Die hier vorgestellte Analysesoftware unterliegt einer Reihe von
Einschränkungen.

Zunächst ist sie in der Verarbeitung der Eingabeparameter beschränkt.
Die Revisionsgeschichte der Repositories muss in der Git
Versionsverwaltung vorliegen. Git hat laut dem Black Duck Open Hub
Repository bei Open Source Projekten zwar einen Marktanteil von
73\%\footcite[Vgl. ][]{CompareRepositoriesOpen}, jedoch wäre eine Unterstützung
für andere Versionskontrollsysteme auch erstrebenswert. Das
Eingabeformat der Aufwandsabschätzungen ist zwar auf eine CSV-Datei
beschränkt, jedoch ist CSV ein offener Standard und die meisten
Tabellenformate lassen sich in dieses Format konvertieren.

Eine weitere Einschränkung der Analysesoftware liegt in der Verarbeitung
der Daten. Durch die Normalisierung aller Zeitreihen gehen die absoluten
Werte verloren und die Komplexitätsmetriken können nur in Relation
zueinander betrachtet werden. Diese Einschränkung ist notwendig, um die
Werte vergleichbar zu machen.

